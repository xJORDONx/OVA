# OVA
Voice_Assistant

# AI Chatbot with Real-time Voice using LLaMA & Streamlit

## ğŸš€ Overview
This project is an AI chatbot with real-time voice capabilities using:
- **LLaMA (llama.cpp)** for optimized inference
- **VITS (TTS API)** for real-time text-to-speech conversion
- **Streamlit** for an interactive web-based chat interface

## ğŸ“Œ Features
âœ… **Chat with AI:** Type your messages and get AI-generated responses.
âœ… **Real-time Voice Output:** AI responses are converted into speech instantly.
âœ… **CUDA-Optimized Processing:** Fully leverages NVIDIA GPUs for faster inference.
âœ… **Maintains Chat History:** Keeps track of conversations dynamically.
âœ… **User-Friendly UI:** Powered by Streamlit for a seamless chat experience.

## ğŸ› ï¸ Installation
### **1ï¸âƒ£ Clone the Repository**
```sh
git clone https://github.com/xJORDONx/OVA.git
cd OVA
```

### **2ï¸âƒ£ Set Up Virtual Environment (Optional but Recommended)**
```sh
python -m venv venv  # Create a virtual environment
source venv/bin/activate  # Activate (Mac/Linux)
venv\Scripts\activate  # Activate (Windows)
```

### **3ï¸âƒ£ Install Dependencies**
```sh
pip install -r requirements.txt
```

## ğŸ¯ Usage
### **Run the Streamlit Chatbot**
```sh
streamlit run app.py
```
This will start the chatbot in a web browser where you can interact with it.

## âš™ï¸ Configuration
Modify the `MODEL_PATH` in `app.py` to point to your LLaMA model file:
```python
MODEL_PATH = "Llama-2-7B-Chat-GGUF/llama-2-7b-chat.Q4_K_S.gguf"
```

## ğŸ–¥ï¸ How It Works
1. User types a message in the chat input.
2. LLaMA generates a response based on the input.
3. The response is converted into speech using the VITS TTS model.
4. The speech is played in real-time using PyAudio.
5. The chat history is updated dynamically in Streamlit.

## ğŸ—ï¸ Tech Stack
- **LLaMA (llama.cpp)** - Optimized AI inference
- **VITS TTS** - Fast text-to-speech processing
- **Streamlit** - Web UI for user interaction
- **PyAudio** - Real-time audio playback
- **Torch (CUDA-enabled)** - Accelerated AI computation

## ğŸ“Œ Example Interaction
```
User: Hello AI!
AI: Hello! How can I assist you today?
(Audio plays AI's response)
```

## ğŸ› ï¸ Troubleshooting
- **Issue: CUDA is not detected?**
  - Ensure you have installed PyTorch with CUDA support: `pip install torch --index-url https://download.pytorch.org/whl/cu118`
  - Verify GPU support with `torch.cuda.is_available()`.
- **Issue: Model is too slow?**
  - Try reducing `max_tokens` or `n_ctx` in `app.py`.

## ğŸ“œ License
This project is open-source and licensed under the MIT License.

## ğŸŒŸ Contributing
Pull requests and issues are welcome! Feel free to enhance the chatbot's capabilities.

