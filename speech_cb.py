import streamlit as st
import torch
import numpy as np
import torchaudio
import pyaudio
from llama_cpp import Llama  # âœ… Optimized LLaMA Inference
from TTS.api import TTS  # âœ… VITS for Real-time Speech

import os
os.environ["CUDA_VISIBLE_DEVICES"] = "0"
os.environ["LLAMA_FORCE_GPU"] = "1"

# **CONFIGURATION**
MODEL_PATH = "C:/Users/vishu/.lmstudio/models/TheBloke/Llama-2-7B-Chat-GGUF/llama-2-7b-chat.Q4_K_S.gguf"

# âœ… **Load Optimized LLaMA Model (Using More GPU)**
llm = Llama(
    model_path=MODEL_PATH,
    n_gpu_layers=150,  # ðŸ”¥ Fully offload to GPU (RTX 4080 can handle it)
    n_ctx=4096,        # âœ… Increase context for better understanding
    n_batch=256,       # âœ… Adjust batch size to avoid VRAM overflow
    use_mmap=True,
    use_mlock=True,
    use_trt=True
)

# âœ… **Enable CUDA Optimizations**
torch.backends.cuda.matmul.allow_tf32 = True  # ðŸ”¥ Enables TF32 for FASTER GPU Compute
torch.backends.cudnn.benchmark = True  # âœ… Optimize CUDA Kernel Selection
torch.cuda.synchronize()  # ðŸ›  Ensure CUDA Kernels Are Efficient

# âœ… **Load VITS TTS Model for Faster Speech**
tts = TTS(model_name="tts_models/en/ljspeech/vits").to("cuda" if torch.cuda.is_available() else "cpu")

# âœ… **Streamlit UI**
st.set_page_config(page_title="Chatbot with Voice", layout="wide")
st.title("ðŸ’¬ AI Chatbot with Real-time Voice")

# âœ… **Maintain Chat History**
if "messages" not in st.session_state:
    st.session_state.messages = []

# âœ… **Display Chat History**
chat_container = st.container()
with chat_container:
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

# âœ… **Generate AI Response with LLaMA**
def generate_response(user_input):
    try:
        # **Format Prompt**
        prompt = f"""### Instruction:
        You are a friendly AI assistant. Respond in a warm, engaging way.

        ### User:
        {user_input}

        ### AI Assistant:"""

        # **Generate Response**
        response = llm(
            prompt, 
            max_tokens=512, 
            temperature=0.8, 
            top_p=0.9, 
            repeat_penalty=1.1
        )
        ai_response = response["choices"][0]["text"].strip() if "choices" in response else "ðŸ¤–: Sorry, I couldn't generate a response."

        # âœ… **Convert Text to Speech (No Saving, Just Play)**
        audio_samples = tts.tts(text=ai_response)
        audio_samples = np.array(audio_samples, dtype=np.float32)  # ðŸ”¥ Fix: Convert List to NumPy Array

        return ai_response, audio_samples

    except Exception as e:
        return f"ðŸ¤–: Error generating response: {str(e)}", None

# âœ… **Real-time Audio Playback Using PyAudio**
def play_audio_realtime(audio_samples, sample_rate=22050):
    """Streams audio directly using PyAudio (Real-time Playback)"""
    p = pyaudio.PyAudio()
    stream = p.open(format=pyaudio.paFloat32, channels=1, rate=sample_rate, output=True)
    stream.write(audio_samples.tobytes())  # âœ… Fix: Convert NumPy array to bytes
    stream.stop_stream()
    stream.close()
    p.terminate()

# âœ… **Chat Interface**
user_input = st.chat_input("Type your message...")

if user_input:
    # âœ… **Append User Message**
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)

    # âœ… **Generate AI Response & Speech**
    response, audio_samples = generate_response(user_input)
    st.session_state.messages.append({"role": "assistant", "content": response})

    # âœ… **Display AI Response & Speak in Real-Time**
    with st.chat_message("assistant"):
        st.markdown(response)

        # âœ… **Play Audio in Real-Time**
        if audio_samples is not None:
            play_audio_realtime(audio_samples)

    # âœ… **Refresh Streamlit Chat**
    st.rerun()
